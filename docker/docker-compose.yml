## UPDATED: 24-04-2022, changelog: https://github.com/zilexa/Homeserver/blob/master/docker/changelog.md
##
## COMMON COMMANDS:
# Check for typos: docker-compose config 
# Run: docker-compose up -d 
# Stop a container: docker-compose stop containername
# Remove a stopped container: docker-compose rm containername 
# Stop all containers: docker kill $(docker ps -q)
# Remove stopped containers: docker container prune
# Stop & remove containers: docker rm $(docker ps -a -q) 
# Remove everything related to stopped containers: sudo docker system prune --all --volumes --force
#
## Docker-Compose 2.4 the final non-swarm version
#
version: "2.4"
services:
##_____MANAGEMENT_____
##____________________ Portainer [Management/Docker]
  portainer:
    container_name: portainer
    image: portainer/portainer-ce
    restart: always
    networks: 
      - web-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - $DOCKERDIR/portainer/data:/data
    ports:
      - 9000:9000
    labels:
      caddy: http://services.o
      caddy.reverse_proxy: "{{upstreams 9000}}"
      flame.type: app
      flame.name: Container Management
      flame.url: http://services.o/
      #flame.icon: custom
##
##____________________ Flame [Management/Homepage]
  flame:
    container_name: homepage
    image: pawelmalak/flame
    restart: always
    networks: 
      - web-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - $DOCKERDIR/homepage/data:/app/data
    environment:
      - PASSWORD=$PW
    ports:
      - 5005:5005
    labels:
      caddy: http://start.o
      caddy.reverse_proxy: "{{upstreams 5005}}"
##
##____________________ Guacamole [Management/remote-desktop]
  guacamole:
    container_name: guacamole
    image: oznu/guacamole
    restart: always
    networks: 
      - web-proxy
    environment:
      EXTENSIONS: auth-quickconnect # add ,auth-totp if exposed to the internet, for 2FA
    volumes:
      - $DOCKERDIR/guacamole:/config
    ports:
     - 8000:8080
    labels:
      caddy: http://desktop.o
      caddy.reverse_proxy: "{{upstreams 8080}}"
      flame.type: app
      flame.name: Remote Desktop
      flame.url: http://desktop.o/
      #flame.icon: custom
##
##____________________ VPN-portal [Management/VPN]
  VPN-portal:
    container_name: vpn-portal
    image: ngoduykhanh/wireguard-ui:latest
    restart: always
    cap_add:
      - NET_ADMIN
    network_mode: host
    environment:
      SMTP_HOSTNAME: $SMTP
      SMTP_PORT: $SMTPPORT
      SMTP_USERNAME: $SMTPUSER
      SMTP_PASSWORD: $SMTPPASS
      SMTP_AUTH_TYPE: LOGIN
      EMAIL_FROM_ADDRESS: $RECIPIENT
      EMAIL_FROM_NAME: $SMTPUSER
      SESSION_SECRET: HmJg1j3pUN85GcfSiQrfrecgqU8oP8eufy7VWxltbfQ=
    logging:
      driver: json-file
      options:
        max-size: 15m
    volumes:
      - $DOCKERDIR/vpn-portal/db:/app/db
      - /etc/wireguard:/etc/wireguard  
    ports:
      - 5000:5000
    labels:
      caddy: http://vpn.o
      caddy.reverse_proxy: "{{upstreams 5000}}"
      flame.type: app
      flame.name: VPN portal
      flame.url: http://vpn.o/
      #flame.icon: custom
##
##____________________ Scrutiny [Management/monitoring]
  scrutiny:
    container_name: scrutiny
    image: cr.hotio.dev/hotio/scrutiny
    restart: always
    cap_add:
      - SYS_RAWIO
      - SYS_ADMIN
    devices:
      - "/dev/nvme0n1"
      - "/dev/sda"
      - "/dev/sdb"
      - "/dev/sdc"
      - "/dev/sdd"
      - "/dev/sde" 
    networks: 
      - web-proxy
    #entrypoint: CMD
    environment:
      mode: web
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
      API_ENDPOINT: http://scrutiny-web:8080
    volumes:
      - /run/udev:/run/udev:ro
      - $DOCKERDIR/scrutiny/config:/config
    ports:
      - "8081:8080"
    labels:
      caddy: http://storage.o
      caddy.reverse_proxy: "{{upstreams 8080}}"
      flame.type: app
      flame.name: Storage Health
      flame.url: http://storage.o/
      #flame.icon: custom
##
##
##______NETWORK_______
##____________________ AdGuard Home [NETWORK/Adblocking-dnsserver]
  adguard:
    container_name: adguard
    image: adguard/adguardhome
    restart: always
    network_mode: host
    volumes:
       - $DOCKERDIR/adguardhome/work:/opt/adguardhome/work
       - $DOCKERDIR/adguardhome//conf:/opt/adguardhome/conf
    labels:
      caddy: http://adguard.o
      caddy.reverse_proxy: "{{upstreams 3000}}"
      flame.type: app
      flame.name: AdGuard
      flame.url: http://adguard.o/
      #flame.icon: custom
##____________________ Unbound [NETWORK/Adblocking-dnsserver]
  unbound:
    container_name: unbound
    image: mvance/unbound:latest
    restart: always
    healthcheck:
      interval: 5m # to test the container, change to 10s. To prevent constant logfile activity, change to a few minutes
      timeout: 3s
      start_period: 5s
    networks: 
      - unbound
    volumes:
      - $DOCKERDIR/unbound/forward-records.conf:/opt/unbound/etc/unbound/forward-records.conf
    ports:
      - 5335:53/tcp
      - 5335:53/udp
##____________________ Castblock [NETWORK/Adblocking-chromecast]
  castblock:
    container_name: blockcast
    image: erdnaxeli/castblock:latest
    restart: always
    network_mode: host
    cap_add: 
      - NET_ADMIN
    environment:
      DEBUG: true
      OFFSET: 1
      CATEGORIES: sponsor,interaction
      MUTE_ADS: true
##
##____________________ Unifi Controller [NETWORK/Wifi]
  unifi:
    container_name: unifi
    image: ghcr.io/linuxserver/unifi-controller
    #mac_address: d0:ca:ab:cd:ef:03
    networks: 
      - web-proxy
    environment:
      PUID: $PUID
      PGID: $PGID
      MEM_LIMIT: 512M #optional
    volumes:
      - $DOCKERDIR/unifi/config:/config
    ports:
      - 10001:10001/udp #to auto-discover your Access Points
      - 8080:8080 # Controller communication with Access Points and vice versa
      - 8443:8443 #to access webUI
      - 1900:1900/udp #to auto-discover this Controller in your app
    labels:
      caddy: http://unifi.o
      caddy.reverse_proxy: "{{upstreams 8443}}"
      flame.type: app
      flame.name: Unifi WiFi
      flame.url: http://unifi.o/
      #flame.icon: custom
##
##
##________CLOUD________
##_____________________ Caddy [CLOUD/web-proxy]
  caddy:
    container_name: web-proxy
    image: lucaslorentz/caddy-docker-proxy:ci-alpine
    restart: always
    networks: 
      - web-proxy
    environment:
      - CADDY_INGRESS_NETWORKS=web-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - $DOCKERDIR/caddy/caddy_data:/data
      - $DOCKERDIR/caddy/config:/config
    labels:
      caddy.email: $EMAIL
      #caddy.log.output: file /data/access.log
    ports:
      - 443:443
      - 80:80
##
##_____________________ Firefox Sync [CLOUD/Browser] 
# generate secret.txt first see docker-config.sh
  firefox-sync:
    container_name: firefox-sync
    image: crazymax/firefox-syncserver:latest
    restart: always
    networks: 
      - web-proxy
    environment: 
      FF_SYNCSERVER_PUBLIC_URL: https://firefox.$DOMAIN
      FF_SYNCSERVER_SECRET: $FFSYNCSECRET
      FF_SYNCSERVER_FORWARDED_ALLOW_IPS: '*'
      FF_SYNCSERVER_FORCE_WSGI_ENVIRON: true
      FF_SYNCSERVER_ALLOW_NEW_USERS: false
      FF_SYNCSERVER_LOGLEVEL: debug
      FF_SYNCSERVER_ACCESSLOG: true
    volumes:
      - $DOCKERDIR/firefox-sync:/data
    labels:
      caddy: firefox.$DOMAIN
      caddy.reverse_proxy: "{{upstreams 5000}}"
##
##_____________________ Bitwarden [CLOUD/Password-manager] 
  vaultwarden:
    container_name: vaultwarden
    image: vaultwarden/server
    restart: always
    healthcheck:
      interval: 5m # to test the container, change to 10s. To prevent constant logfile activity, change to a few minutes
    networks: 
      - web-proxy
    volumes:
      - $DOCKERDIR/vaultwarden:/data
    environment:
      WEBSOCKET_ENABLED: true
      DOMAIN: vault.$DOMAIN
      SIGNUPS_ALLOWED: false
      ADMIN_TOKEN: $VAULTWARDENTOKEN
    labels:
      caddy: vault.$DOMAIN
      caddy.reverse_proxy_0: "{{upstreams 80}}"
      # Required extra headers
      caddy.encode: gzip
      caddy.header.X-XSS-Protection: '"1; mode=block;"'
      caddy.header.X-Frame-Options: "DENY"
      caddy.header.X-Content-Type-Options: "none"
      caddy.reverse_proxy_1: "/notifications/hub/negotiate {{upstreams 80}}"
      caddy.reverse_proxy_2: "/notifications/hub {{upstreams 3012}}"
##
##____________________ FileRun [CLOUD/FileRun]
  filerun:
    container_name: filerun
    image: filerun/filerun
    restart: always
    networks: 
      - web-proxy
      - filerun
    environment:
      FR_DB_HOST: filerun-db
      FR_DB_PORT: 3306
      FR_DB_NAME: filerundb
      FR_DB_USER: $USER
      FR_DB_PASS: $PW_DB
      APACHE_RUN_USER: $USER
      APACHE_RUN_USER_ID: $PUID
      APACHE_RUN_GROUP: $USER
      APACHE_RUN_GROUP_ID: $PGID
    depends_on:
      - filerun-db
    volumes:
      - $DOCKERDIR/filerun/html:/var/www/html
      - $DATAPOOL/Users:/user-files
    labels:
      caddy: files.$DOMAIN
      caddy.reverse_proxy: "{{upstreams 80}}"
      caddy.reverse_proxy.header_up: "Host files.$DOMAIN"
      # Required extra headers
      caddy.file_server: ""                                         # required for fileservers
      caddy.encode: gzip                                            # required for fileservers
      caddy.header.Strict-Transport-Security: '"max-age=15768000;"' # Recommended security hardening for fileservers
      caddy.header.X-XSS-Protection: '"1; mode=block;"'             # Recommended security hardening for fileservers
      caddy.header.X-Content-Type-Options: "nosniff"                # Seems required to open files in OnlyOffice
      caddy.header.X-Frame-Options: "SAMEORIGIN"                    # Seems required to open files in OnlyOffice
##____________________ Filerun database [CLOUD/FileRun/db]
  filerun-db:
    container_name: filerun-db
    image: mariadb:10.1
    restart: always
    networks:
      - filerun
    environment:
      MYSQL_ROOT_PASSWORD: $PW_DB
      MYSQL_USER: $USER
      MYSQL_PASSWORD: $PW_DB
      MYSQL_DATABASE: filerundb
    volumes:
      - $DOCKERDIR/filerun/db:/var/lib/mysql
##_____________________ OnlyOffice Document Server [Cloud/Office]
  onlyoffice:
    container_name: onlyoffice
    image: onlyoffice/documentserver
    stdin_open: true
    restart: always
    networks: 
      - web-proxy
    tty: true
    volumes:
      - $DOCKERDIR/onlyoffice/data:/var/www/onlyoffice/Data
      - $DOCKERDIR/onlyoffice/log:/var/log/onlyoffice
      - $DOCKERDIR/onlyoffice/database:/var/lib/postgresql
      - /usr/share/fonts:/usr/share/fonts
    environment:
      JWT_ENABLED: true
      JWT_SECRET: $ONLYOFFICEJWT
    labels:
      caddy: office.$DOMAIN
      caddy.reverse_proxy: "{{upstreams 80}}"
      caddy.encode: gzip
      # Required extra headers
      #caddy.file_server: ""
      #caddy.header.X-Content-Type-Options: "nosniff"
      #caddy.tls_insecure_skip_verify: ""
      #caddy.header: /
##
##
##________MEDIA________
##_____________________ Jellyfin [MEDIA/Library] 
  jellyfin:
    container_name: jellyfin
    image: cr.hotio.dev/hotio/jellyfin
    restart: always
    networks: 
      - web-proxy
    # Required for Intel QuickSync/VAAPI hardware accelerated video encoding/transcoding
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    environment:
      PUID: $PUID
      PGID: $PGID
      TZ: $TZ
      UMASK_SET: 002 #optional
    volumes:
      - $DOCKERDIR/jellyfin/config:/config
      - $DATAPOOL/Media:/data
    ports:
      - 8096:8096
    labels:
      caddy: http://jellyfin.o
      caddy.reverse_proxy: "{{upstreams 8096}}"
      flame.type: app
      flame.name: Jellyfin
      flame.url: http://jellyfin.o/
      #flame.icon: custom
      org.hotio.pullio.update: true

##____________________ VPN-proxy [MEDIA/vpn-client-for-media]
  VPN-proxy:
    container_name: VPN-proxy
    image: thrnz/docker-wireguard-pia
    restart: always
    networks: 
      - web-proxy
    cap_add:
      - NET_ADMIN
      #- SYS_MODULE might not be needed with a 5.6+ kernel?
      #- SYS_MODULE
      # Mounting the tun device may be necessary for userspace implementations
      #devices:
      #- /dev/net/tun:/dev/net/tun
    privileged: true
    sysctls:
      # wg-quick fails to set this without --privileged, so set it here instead if needed
      - net.ipv4.conf.all.src_valid_mark=1
      # May as well disable ipv6. Should be blocked anyway.
      - net.ipv6.conf.default.disable_ipv6=1
      - net.ipv6.conf.all.disable_ipv6=1
      - net.ipv6.conf.lo.disable_ipv6=1
    healthcheck:
      test: ping -c 1 www.google.com || exit 1
      interval: 5m # While testing container, change to 10s. To prevent constant logfile activity, change to a few minutes
      timeout: 10s
      start_period: 10s
      retries: 3
    environment:
      LOCAL_NETWORK: 192.168.88.0/24,10.6.0.1/24
      LOC: de-frankfurt
      USER: $VPN_USER_PIA
      PASS: $VPN_PW_PIA
      #KEEPALIVE: 25
      #VPNDNS: 8.8.8.8,8.8.4.4
      PORT_FORWARDING: 1
      PORT_PERSIST: 0
      PORT_SCRIPT: /pia-shared/updateport-qbittorrent.sh
      #WG_USERSPACE: 1
    volumes:
      # Auth token is stored here
      - $DOCKERDIR/vpn-proxy/pia:/pia
      # If enabled, the forwarded port is dumped to /pia-shared/port.dat for potential use in other containers
      - $DOCKERDIR/vpn-proxy/pia-shared:/pia-shared
    # The container has no recovery logic. Use a healthcheck to catch disconnects.
    ports:
      - 9090:8080 #Qbittorrent webUI
    labels:
      caddy: http://downloads.o
      caddy.reverse_proxy: "{{upstreams 8080}}"
      flame.type: app
      flame.name: Downloads
      flame.url: http://downloads.o/
      #flame.icon: custom
      org.hotio.pullio.update: true
##
##____________________ Transmission [MEDIA/download-client]
  qbittorrent:
    container_name: qbittorrent
    image: cr.hotio.dev/hotio/qbittorrent
    depends_on:
      - VPN-proxy
    network_mode: service:VPN-proxy
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      TZ: $TZ
    volumes:
      - $DOCKERDIR/qbittorrent:/config
      - $DATAPOOL/Media/incoming:/Media/incoming
    labels: 
      org.hotio.pullio.update: true
##
##____________________ Prowlarr [MEDIA/torrent-proxy for Sonarr&Radarr]
  prowlarr:
    container_name: prowlarr
    image: cr.hotio.dev/hotio/prowlarr:testing
    networks: 
      - web-proxy
    depends_on:
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/prowlarr/config:/config
      - $DATAPOOL/Media/incoming:/Media/incoming
    ports:
      - 9696:9696
    labels:
      caddy: http://torrents.o
      caddy.reverse_proxy: "{{upstreams 9696}}"
      flame.type: app
      flame.name: Search Torrents
      flame.url: http://torrents.o/
      #flame.icon: custom
      org.hotio.pullio.update: true
##
##____________________ Sonarr [MEDIA/PVR-TVshows]
  sonarr:
    container_name: sonarr
    image: cr.hotio.dev/hotio/sonarr
    networks: 
      - web-proxy
    depends_on:
      - prowlarr
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/sonarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 8989:8989
    labels:
      caddy: http://sonarr.o
      caddy.reverse_proxy: "{{upstreams 8989}}"
      flame.type: app
      flame.name: Sonarr (series)
      flame.url: http://sonarr.o/
      #flame.icon: custom
      org.hotio.pullio.update: true
##
##____________________ Radarr [MEDIA/PVR-Movies]
  radarr:
    container_name: radarr
    image: cr.hotio.dev/hotio/radarr
    networks: 
      - web-proxy
    depends_on:
      - prowlarr
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/radarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 7878:7878
    labels:
      caddy: http://radarr.o
      caddy.reverse_proxy: "{{upstreams 7878}}"
      flame.type: app
      flame.name: Radarr (movies)
      flame.url: http://radarr.o/
      #flame.icon: custom
      org.hotio.pullio.update: true
##
##____________________ Bazarr [MEDIA/subtitles]
  bazarr:
    container_name: bazarr
    image: cr.hotio.dev/hotio/bazarr
    networks: 
      - web-proxy
    depends_on:
       - sonarr
       - radarr
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/bazarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 6767:6767
    labels:
      caddy: http://bazarr.o
      caddy.reverse_proxy: "{{upstreams 6767}}"
      flame.type: app
      flame.name: Bazarr (subtitles)
      flame.url: http://bazarr.o/
      #flame.icon: custom
      org.hotio.pullio.update: true
##
##____________________ Lidarr [MEDIA/PVR-Music]
  lidarr:
    container_name: lidarr
    image: cr.hotio.dev/hotio/lidarr
    networks: 
      - web-proxy
    depends_on:
      - prowlarr
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/lidarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 8686:8686
    labels:
      caddy: http://lidarr.o
      caddy.reverse_proxy: "{{upstreams 8686}}"
      flame.type: app
      flame.name: Lidarr (music)
      flame.url: http://lidarr.o/
      #flame.icon: custom
      org.hotio.pullio.update: true
#
#
networks:
  web-proxy:
    external: true
  filerun:
    driver: bridge
  unbound:
    driver: bridge
